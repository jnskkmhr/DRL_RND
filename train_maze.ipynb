{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ad81f5",
   "metadata": {},
   "source": [
    "# Random Network Distillation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059824eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video, display, clear_output\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "import gymnasium_robotics\n",
    "\n",
    "# torch default device\n",
    "if  torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"mps\")\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from rnd_rl.runner.policy_runner import PPOConfig, PolicyRunner\n",
    "\n",
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization code. Used later.\n",
    "fixed_goal_maze =  [[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, \"g\", 0, 1, 1, 0, 0, 1],\n",
    "            [1, 0, 0, 1, 0, 0, 0, 1],\n",
    "            [1, 1, 0, 0, 0, 1, 1, 1],\n",
    "            [1, 0, 0, 1, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "            [1, 0, 0, 0, 1, \"r\", 0, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1]] # force a longer path.\n",
    "\n",
    "def visualize(agent):\n",
    "\n",
    "    video_dir = \"./videos\"  # Directory to save videos\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    # Create environment with proper render_mode\n",
    "    # env = gym.make(\"PointMaze_UMaze\", render_mode=\"rgb_array\", maze_map = fixed_goal_maze, continuing_task = False)\n",
    "    env = gym.make(\"PointMaze_Medium-v3\", render_mode=\"rgb_array\", maze_map = fixed_goal_maze, continuing_task = False)\n",
    "\n",
    "    # Apply video recording wrapper\n",
    "    env = RecordVideo(env, video_folder=video_dir, episode_trigger=lambda x: True)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    obs = obs[\"observation\"]\n",
    "\n",
    "    for t in range(4096):\n",
    "        actions, _ = agent.get_action(torch.Tensor(obs)[None, :].to(device))\n",
    "        obs, _, done, _, _ = env.step(actions.squeeze(0).cpu().numpy())\n",
    "        obs = obs[\"observation\"]\n",
    "\n",
    "        if done:\n",
    "            # self.writer.add_scalar(\"Duration\", t, i)\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Display the latest video\n",
    "    video_path = os.path.join(video_dir, sorted(os.listdir(video_dir))[-1])  # Get the latest video\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(Video(video_path, embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 64\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: gym.make(\"PointMaze_Medium-v3\", maze_map = fixed_goal_maze, continuing_task = False) for _ in range(n_envs)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6fb40",
   "metadata": {},
   "source": [
    "### PPO baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_cfg = PPOConfig(\n",
    "    use_rnd=False, \n",
    "    clip_params=0.2,\n",
    "    init_noise_std=1.0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dea9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "policy_runner = PolicyRunner(\n",
    "    envs=envs, \n",
    "    policy_cfg=ppo_cfg, \n",
    "    num_mini_epochs=10, \n",
    "    device=device, \n",
    "    experiment_name=\"PPO_maze_med_far\",\n",
    "    dict_obs_space = True,\n",
    "    num_steps_per_env = 256 # 128 # makes it difficult for the agent to reach goal\n",
    ")\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    policy_runner.rollout(epoch)\n",
    "    policy_runner.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b11d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(policy_runner.alg)\n",
    "print(\"PPO trained agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920db6d5",
   "metadata": {},
   "source": [
    "### PPO with RND\n",
    "Finding: if the intrinsic rewards always dominates, then agent cannot improve - does not know which action can lead to the true extrinsic reward. Requires reward normalization (which is only applied to intrinsic rewards, zero-centered.) Maybe also need to rescale. The original paper worked because game scores are much larger than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb72bcb",
   "metadata": {},
   "source": [
    "### Reward normalization only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppo_rnd_reward_normalization_cfg = PPOConfig(\n",
    "    use_rnd=True, \n",
    "    clip_params=0.2,\n",
    "    init_noise_std=1.0, \n",
    "    reward_normalization = True,\n",
    "    intrinsic_reward_scale = 0.1 # excessive intrinsic reward might degrade training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40313c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250 \n",
    "rnd_reward_norm_policy_runner = PolicyRunner(\n",
    "    envs=envs, \n",
    "    policy_cfg=ppo_rnd_reward_normalization_cfg, \n",
    "    num_mini_epochs=10,\n",
    "    device=device, \n",
    "    experiment_name=\"PPO_RND_rew_norm_maze_med_far\",\n",
    "    dict_obs_space = True,\n",
    "    num_steps_per_env = 256 # 128 # makes it difficult for the agent to reach goal\n",
    ")\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    rnd_reward_norm_policy_runner.rollout(epoch)\n",
    "    rnd_reward_norm_policy_runner.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe857d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(rnd_reward_norm_policy_runner.alg)\n",
    "print(\"RND PPO trained agent with reward normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode length\n",
    "# print(torch.nonzero(rnd_reward_norm_policy_runner.traj_data.extrinsic_rewards)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude = torch.sqrt(torch.pow(rnd_reward_norm_policy_runner.traj_data.states[2],2) \\\n",
    "    + torch.pow(rnd_reward_norm_policy_runner.traj_data.states[3],2))\n",
    "\n",
    "print(torch.mean(magnitude))\n",
    "print(torch.max(magnitude))\n",
    "\n",
    "# print(magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8df8e",
   "metadata": {},
   "source": [
    "### Reward and observation normalization\n",
    "Finding: might work better for the original implementation with image embedding observations of unknown value range. Here, it will only improve if the agent can reach to most of the obs spaces in the first several steps, which is not applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e029b7",
   "metadata": {},
   "source": [
    "### Reward and observation normalization\n",
    "Finding: might work better for the original implementation with image embedding observations of unknown value range. Here, it will only improve if the agent can reach to most of the obs spaces in the first several steps, which is not applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
