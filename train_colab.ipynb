{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ad81f5",
   "metadata": {},
   "source": [
    "# Random Network Distillation - colab version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4756bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altered the original notebook for colab\n",
    "# The only difference is the first 3 cells.\n",
    "! git clone https://github.com/jnskkmhr/DRL_RND.git\n",
    "%cd DRL_RND\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video, display, clear_output\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# torch default device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "from rnd_rl.runner.policy_runner import PPOConfig, PolicyRunner\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization code. Used later.\n",
    "\n",
    "def visualize(agent):\n",
    "\n",
    "    video_dir = \"./videos\"  # Directory to save videos\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    # Create environment with proper render_mode\n",
    "    env = gym.make(\"InvertedPendulum-v5\", render_mode=\"rgb_array\", reset_noise_scale=0.2)\n",
    "\n",
    "    # Apply video recording wrapper\n",
    "    env = RecordVideo(env, video_folder=video_dir, episode_trigger=lambda x: True)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "\n",
    "    for t in range(4096):\n",
    "        actions, _ = agent.get_action(torch.Tensor(obs)[None, :].to(device))\n",
    "        obs, _, done, _ = env.step(actions.squeeze(0).cpu().numpy())\n",
    "\n",
    "        if done:\n",
    "            # self.writer.add_scalar(\"Duration\", t, i)\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Display the latest video\n",
    "    video_path = os.path.join(video_dir, sorted(os.listdir(video_dir))[-1])  # Get the latest video\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(Video(video_path, embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 64\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: gym.make(\"InvertedPendulum-v5\", reset_noise_scale=0.2) for _ in range(n_envs)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6fb40",
   "metadata": {},
   "source": [
    "### PPO baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_cfg = PPOConfig(\n",
    "    use_rnd=False, \n",
    "    clip_params=0.2,\n",
    "    init_noise_std=1.0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dea9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "policy_runner = PolicyRunner(envs=envs, policy_cfg=ppo_cfg, num_mini_epochs=10, device=device)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    policy_runner.rollout(epoch)\n",
    "    policy_runner.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b11d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(policy_runner.alg)\n",
    "print(\"PPO trained agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920db6d5",
   "metadata": {},
   "source": [
    "### PPO with RND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_rnd_cfg = PPOConfig(\n",
    "    use_rnd=True, \n",
    "    clip_params=0.2,\n",
    "    init_noise_std=1.0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250 \n",
    "rnd_policy_runner = PolicyRunner(envs=envs, policy_cfg=ppo_rnd_cfg, num_mini_epochs=10,device=device)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    rnd_policy_runner.rollout(epoch)\n",
    "    rnd_policy_runner.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7daf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(rnd_policy_runner.alg)\n",
    "print(\"RND PPO trained agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb72bcb",
   "metadata": {},
   "source": [
    "### Reward normalization only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppo_rnd_reward_normalization_cfg = PPOConfig(\n",
    "    use_rnd=True, \n",
    "    clip_params=0.2,\n",
    "    init_noise_std=1.0, \n",
    "    reward_normalization = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40313c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250 \n",
    "rnd_reward_norm_policy_runner = PolicyRunner(envs=envs, policy_cfg=ppo_rnd_reward_normalization_cfg, num_mini_epochs=10,device=device)\n",
    "rnd_reward_norm_policy_runner.writer = SummaryWriter(log_dir=f'runs/{\"RND_reward_normalization\"}') \n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    rnd_reward_norm_policy_runner.rollout(epoch)\n",
    "    rnd_reward_norm_policy_runner.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe857d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(rnd_reward_norm_policy_runner.alg)\n",
    "print(\"RND PPO trained agent with reward normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8df8e",
   "metadata": {},
   "source": [
    "### Reward and observation normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b10ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_rnd_all_normalization_cfg = PPOConfig(\n",
    "    use_rnd=True, \n",
    "    clip_params=0.2,\n",
    "    init_noise_std=1.0, \n",
    "    reward_normalization = True,\n",
    "    obs_normalization = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fdb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250 \n",
    "rnd_all_norm_policy_runner = PolicyRunner(envs=envs, policy_cfg=ppo_rnd_all_normalization_cfg, num_mini_epochs=10,device=device)\n",
    "rnd_all_norm_policy_runner.writer = SummaryWriter(log_dir=f'runs/{\"RND_all_normalization\"}') \n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    rnd_all_norm_policy_runner.rollout(epoch)\n",
    "    rnd_all_norm_policy_runner.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(rnd_all_norm_policy_runner.alg)\n",
    "print(\"RND PPO trained agent with observation normalization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
